{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"speech-separation_ja.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMGQ+rOFXcE/1VwUlO7R3vm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y4o7K9HrAiT5"},"source":["# 事前学習済みモデルによる話者分離"]},{"cell_type":"code","metadata":{"id":"o6WN7Ho2Aamf"},"source":["%%bash\n","git clone https://github.com/tky823/DNN-based_source_separation.git\n","\n","cd \"./DNN-based_source_separation/egs/tutorials/\"\n","\n","# torchとtorchaudioのインストール\n","pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Msllhj-LAoAG"},"source":["%%bash\n","# データセットのダウンロード\n","for spk in aew axb bdl ; do\n","    wget \"http://festvox.org/cmu_arctic/packed/cmu_us_${spk}_arctic.tar.bz2\"\n","    tar -xjvf \"./cmu_us_${spk}_arctic.tar.bz2\" \n","done"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RuuAPjsApFv"},"source":["import sys"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LyAkk_ahRhR"},"source":["sys.path.append(\"/content/DNN-based_source_separation/src\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BT0uC_w_AqLQ"},"source":["import IPython.display as ipd\n","import torch\n","import torchaudio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eB3bUXkhcxeA"},"source":["torch.manual_seed(111)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DaHj1YSRArJt"},"source":["from models.deep_clustering import DeepClustering\n","from models.danet import DANet, FixedAttractorDANet\n","from models.adanet import ADANet\n","from models.lstm_tasnet import LSTMTasNet\n","from models.conv_tasnet import ConvTasNet\n","from models.dprnn_tasnet import DPRNNTasNet\n","from models.dptnet import DPTNet\n","from models.sepformer import SepFormer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muHyR-amAsKU"},"source":["waveform_aew, sample_rate = torchaudio.load(\"/content/cmu_us_aew_arctic/wav/arctic_a0001.wav\")\n","waveform_axb, sample_rate = torchaudio.load(\"/content/cmu_us_axb_arctic/wav/arctic_a0002.wav\")\n","waveform_bdl, sample_rate = torchaudio.load(\"/content/cmu_us_bdl_arctic/wav/arctic_a0003.wav\")\n","SAMPLE_RATE_WSJ0 = 8000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcgxVjzqAtLJ"},"source":["resampler = torchaudio.transforms.Resample(sample_rate, SAMPLE_RATE_WSJ0)\n","waveform_aew = resampler(waveform_aew)\n","waveform_axb = resampler(waveform_axb)\n","waveform_bdl = resampler(waveform_bdl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gt6nQrxiAuPw"},"source":["T_min = min(waveform_aew.size(-1), waveform_axb.size(-1), waveform_bdl.size(-1))\n","waveform_aew, waveform_axb, waveform_bdl = waveform_aew[:, :T_min], waveform_axb[:, :T_min], waveform_bdl[:, :T_min]\n","display(ipd.Audio(waveform_aew, rate=SAMPLE_RATE_WSJ0))\n","display(ipd.Audio(waveform_axb, rate=SAMPLE_RATE_WSJ0))\n","display(ipd.Audio(waveform_bdl, rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W13n8Xq1AwpZ"},"source":["## 2話者"]},{"cell_type":"code","metadata":{"id":"ZZLv1UDVAvSB"},"source":["n_sources = 2\n","mixture = waveform_aew + waveform_axb\n","display(ipd.Audio(mixture, rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = DeepClustering.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","wrapper_model = DeepClustering.TimeDomainWrapper(model, n_fft=model.n_fft, hop_length=model.hop_length, window_fn=model.window_fn)\n","wrapper_model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = wrapper_model(input, threshold=model.threshold, n_sources=n_sources, iter_clustering=None)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"metadata":{"id":"HrvdSi2KO7zn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0jy3WoKiIlC"},"source":["model = DANet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","wrapper_model = DANet.TimeDomainWrapper(model, n_fft=model.n_fft, hop_length=model.hop_length, window_fn=model.window_fn)\n","wrapper_model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = wrapper_model(input, threshold=model.threshold, n_sources=n_sources, iter_clustering=None)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JwXTR73nN6Hj"},"source":["model = FixedAttractorDANet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","wrapper_model = FixedAttractorDANet.TimeDomainWrapper(model, n_fft=model.n_fft, hop_length=model.hop_length, window_fn=model.window_fn)\n","wrapper_model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = wrapper_model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORVs0wI0ssA1"},"source":["model = ADANet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","wrapper_model = ADANet.TimeDomainWrapper(model, n_fft=model.n_fft, hop_length=model.hop_length, window_fn=model.window_fn)\n","wrapper_model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = wrapper_model(input, threshold=40, n_sources=n_sources)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6uC2_f2gAwb2"},"source":["model = LSTMTasNet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTYGLVPIAzbC"},"source":["model = ConvTasNet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiTCbu41A0sI"},"source":["model = DPRNNTasNet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Mf9vAawA2El"},"source":["model = DPTNet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FM5JxihnhhRt"},"source":["model = SepFormer.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KSUpqEIA4v-"},"source":["## 3話者"]},{"cell_type":"code","metadata":{"id":"VFlyo69yA3Q-"},"source":["n_sources = 3\n","mixture = waveform_aew + waveform_axb + waveform_bdl\n","display(ipd.Audio(mixture, rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = DeepClustering.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","wrapper_model = DeepClustering.TimeDomainWrapper(model, n_fft=model.n_fft, hop_length=model.hop_length, window_fn=model.window_fn)\n","wrapper_model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = wrapper_model(input, threshold=model.threshold, n_sources=n_sources, iter_clustering=None)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"metadata":{"id":"6RClgDf9Jam4"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"weEk4Q8ebtJg"},"source":["model = DANet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","wrapper_model = DANet.TimeDomainWrapper(model, n_fft=model.n_fft, hop_length=model.hop_length, window_fn=model.window_fn)\n","wrapper_model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = wrapper_model(input, threshold=model.threshold, n_sources=n_sources, iter_clustering=None)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaOkvinThYse"},"source":["model = FixedAttractorDANet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","wrapper_model = FixedAttractorDANet.TimeDomainWrapper(model, n_fft=model.n_fft, hop_length=model.hop_length, window_fn=model.window_fn)\n","wrapper_model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = wrapper_model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rxKGHa72-TY"},"source":["model = ADANet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","wrapper_model = ADANet.TimeDomainWrapper(model, n_fft=model.n_fft, hop_length=model.hop_length, window_fn=model.window_fn)\n","wrapper_model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = wrapper_model(input, threshold=40, n_sources=n_sources)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = LSTMTasNet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"metadata":{"id":"XVhE4kIYEP4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6VOXo8-A67E"},"source":["model = ConvTasNet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhV7r6IiA8N3"},"source":["model = DPRNNTasNet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = DPTNet.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"metadata":{"id":"tEJilvvhEHQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"INOjqfGzA9US"},"source":["model = SepFormer.build_from_pretrained(task=\"wsj0-mix\", sample_rate=SAMPLE_RATE_WSJ0, n_sources=n_sources)\n","model.eval()\n","\n","input = mixture.unsqueeze(dim=0)\n","with torch.no_grad():\n","    output = model(input)\n","\n","output = output.squeeze(dim=0)\n","estimated = torch.split(output, [1]*n_sources, dim=0)\n","\n","for idx in range(n_sources):\n","    display(ipd.Audio(estimated[idx], rate=SAMPLE_RATE_WSJ0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvjRu8FG8GmA"},"source":[""],"execution_count":null,"outputs":[]}]}